---
title: "predict_citibike"
output: html_document
date: "2023-06-19"
---

```{r setup, include=FALSE}
library(tidyverse)
library(scales)
library(modelr)

theme_set(theme_bw())
knitr::opts_chunk$set(echo = TRUE)
```

## Use the trips_per_day.tsv file that has one row for each day, the number of trips taken on that day, and the minimum temperature on that day.


```{r load-data}
trips_per_day <- read_tsv('trips_per_day.tsv')
trips_per_day <- trips_per_day %>% 
  mutate(dow = wday(ymd), is_weekend = as.numeric((dow > 5)))

head(trips_per_day)
```

## Split the data into randomly selected training, validation, and test sets, with 90% of the data for training and validating the model, and 10% for a final test set (to be used once and only once towards the end of this exercise). 


```{r data-split-test}
set.seed(42)

num_days <- nrow(trips_per_day)
frac_model <- 0.9
num_model <- floor(num_days * frac_model)

# randomly sample rows for the training set 
ndx <- sample(1:num_days, num_model, replace=F)

# used to fit the model
trips_per_day_model <- trips_per_day[ndx, ]

# used to evaluate the fit
trips_per_day_final <- trips_per_day[-ndx, ]
```

## split the 90% into an 80-20 split

```{r data-split-train-validation}
set.seed(42)

num_days <- nrow(trips_per_day_model)
frac_train <- 0.8
num_train <- floor(num_days * frac_train)

# randomly sample rows for the training set 
ndx <- sample(1:num_days, num_train, replace=F)

# used to fit the model
trips_per_day_train <- trips_per_day_model[ndx, ]

# used to evaluate the fit
trips_per_day_validate <- trips_per_day_model[-ndx, ]
```

## use a single validation fold or k-fold cross-validation if you'd like a more robust estimate.

## Start out with the model in that notebook, which uses only the minimum temperature on each day to predict the number of trips taken that day. Try different polynomial degrees in the minimum temperature and check that you get results similar to what's in that notebook, although they likely won't be identical due to shuffling of which days end up in the train, and validation splits. Quantify your performance using root mean-squared error.

```{r k-fold-cross-validation}
set.seed(42)
num_folds <- 5
num_days <- nrow(trips_per_day)

ndx <- sample(1:num_days, num_train, replace=F)

trips_per_day <- trips_per_day[ndx, ] %>%
  mutate(fold = (row_number() %% num_folds) + 1)

head(trips_per_day)


# fit a model for each polynomial degree
K <- 1:8
avg_validate_err <- c()
se_validate_err <- c()
for (k in K) {

  # do 5-fold cross-validation within each value of k
  validate_err <- c()
  for (f in 1:num_folds) {
    # fit on the training data
    trips_per_day_train <- filter(trips_per_day, fold != f)
    model <- lm(num_trips ~ poly(tmin, k, raw = T), data=trips_per_day_train)

    # evaluate on the validation data
    trips_per_day_validate <- filter(trips_per_day, fold == f)
    validate_err[f] <- sqrt(mean((predict(model, trips_per_day_validate) - trips_per_day_validate$num_trips)^2))
  }

  # compute the average validation error across folds
  # and the standard error on this estimate
  avg_validate_err[k] <- mean(validate_err)
  se_validate_err[k] <- sd(validate_err) / sqrt(num_folds)
}

# plot the validate error, highlighting the value of k with the lowest average error
plot_data <- data.frame(K, avg_validate_err, se_validate_err)
ggplot(plot_data, aes(x=K, y=avg_validate_err)) +
  geom_pointrange(aes(ymin=avg_validate_err - se_validate_err,
                      ymax=avg_validate_err + se_validate_err,
                      color=avg_validate_err == min(avg_validate_err))) +
  geom_line(color = "red") +
  scale_x_continuous(breaks=1:12) +
  theme(legend.position="none") +
  xlab('Polynomial Degree') +
  ylab('RMSE on validation data')

# a fourth degree polynomial is a reasonable choice, but we also see that there's a fair bit of uncertainty on our estimate
     
```

## Now get creative and extend the model to improve it. You can use any features you like that are available prior to the day in question, ranging from the weather, to the time of year and day of week, to activity in previous days or weeks, but don't cheat and use features from the future (e.g., the next day's trips). You can even try adding holiday effects. You might want to look at feature distributions to get a sense of what tranformations (e.g., log or manually created factors such as weekday vs. weekend) might improve model performance. You can also interact features with each other. This formula syntax in R reference might be useful.

## Try a bunch of different models and ideas, documenting them in your Rmarkdown file. Inspect the models to figure out what the highly predictive features are, and see if you can prune away any negligble features that don't matter much. Report the model with the best performance on the validation data. Watch out for overfitting.

```{r}
model <- lm(num_trips ~ poly(tmin, 5, raw = T) * I(prcp > 0), data=trips_per_day_train)


trips_per_day_train <- trips_per_day_train %>%
  add_predictions(model) %>%
  mutate(split = "train")
trips_per_day_validate <- trips_per_day_validate %>%
  add_predictions(model) %>%
  mutate(split = "validate")
plot_data <- bind_rows(trips_per_day_train, trips_per_day_validate)

ggplot(plot_data, aes(x = tmin, y = num_trips)) +
  geom_point(aes(color = split)) +
  geom_line(aes(y = pred, linetype = prcp > 0)) +
  xlab('Minimum temperature') +
  ylab('Daily trips') +
  scale_y_continuous()

plot_data <- data.frame(K, avg_validate_err, se_validate_err)
ggplot(plot_data, aes(x=K, y=avg_validate_err)) +
  geom_pointrange(aes(ymin=avg_validate_err - se_validate_err,
                      ymax=avg_validate_err + se_validate_err,
                      color=avg_validate_err == min(avg_validate_err))) +
  geom_line(color = "red") +
  scale_x_continuous(breaks=1:12) +
  theme(legend.position="none") +
  xlab('Polynomial Degree') +
  ylab('RMSE on validation data')
```

```{r weekend}
K <- 1:8
avg_validate_err <- c()
se_validate_err <- c()
for (k in K) {

  # do 5-fold cross-validation within each value of k
  validate_err <- c()
  for (f in 1:num_folds) {
    # fit on the training data
    trips_per_day_train <- filter(trips_per_day, fold != f)
    model <- lm(num_trips ~ poly(tmin, k, raw = T)*I(prcp > 0) + is_weekend, trips_per_day_train)

    # evaluate on the validation data
    trips_per_day_validate <- filter(trips_per_day, fold == f)
    validate_err[f] <- sqrt(mean((predict(model, trips_per_day_validate) - trips_per_day_validate$num_trips)^2))
  }

  # compute the average validation error across folds
  # and the standard error on this estimate
  avg_validate_err[k] <- mean(validate_err)
  se_validate_err[k] <- sd(validate_err) / sqrt(num_folds)
}
plot_data <- data.frame(K, avg_validate_err, se_validate_err)
ggplot(plot_data, aes(x=K, y=avg_validate_err)) +
  geom_pointrange(aes(ymin=avg_validate_err - se_validate_err,
                      ymax=avg_validate_err + se_validate_err,
                      color=avg_validate_err == min(avg_validate_err))) +
  geom_line(color = "red") +
  scale_x_continuous(breaks=1:12) +
  theme(legend.position="none") +
  xlab('Polynomial Degree') +
  ylab('RMSE on validation data')
```
```{r}
model <- lm(num_trips ~ poly(tmin, 4, raw = T) * I(prcp > 0) + is_weekend, trips_per_day_train)

save(model, file = "predict_citibike.Rdata")

trips_per_day_train <- trips_per_day_train %>%
  add_predictions(model) %>%
  mutate(split = "train")
trips_per_day_validate <- trips_per_day_validate %>%
  add_predictions(model) %>%
  mutate(split = "validate")
plot_data <- bind_rows(trips_per_day_train, trips_per_day_validate)

ggplot(plot_data, aes(x = tmin, y = num_trips)) +
  geom_point(aes(color = split)) +
  geom_line(aes(y = pred, linetype = prcp > 0)) +
  xlab('Minimum temperature') +
  ylab('Daily trips') +
  scale_y_continuous()
```

